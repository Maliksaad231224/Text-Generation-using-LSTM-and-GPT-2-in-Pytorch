{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Text Generation Using Recurrent Neural Networks </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **IMPORTING THE NECESSARY LIBRARIES**\n",
    "<ul>\n",
    "  <li>Pytorch</li>\n",
    "  <li>NLTK</li>\n",
    "  <li>String</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pprint import pprint\n",
    "import string\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **CHARACTER LEVEL PREDICTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"When it comes to generating text, GANs and LSTMs have different approaches. LSTMs excel at capturing sequential patterns and context, making them well-suited for tasks like language translation and text summarization. However, they can struggle with creativity and diversity in their output. On the other hand, GANs are designed to generate novel and diverse text by learning the underlying data distribution. While they can produce more creative content, GANs can be challenging to train and evaluate, and may require additional techniques to ensure coherence and fluency. Ultimately, the choice between GANs and LSTMs depends on the specific text generation task and the desired output: if you need coherent and natural-sounding text, LSTMs might be the better choice, but if you want to generate creative and diverse content, GANs could be the way to go.\"\n",
    "chars=list(set(data))\n",
    "\n",
    "char_to_idx={char:i for i,char in enumerate(chars)}\n",
    "idx_to_char={i:char for i,char in enumerate(chars)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **MAKING CHARACTER PREDICTION MODEL USING RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward pass through LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Pass the last time step's output through a fully connected layer\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **DEFINING THE MODEL, CRITERION AND OPTIMIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(len(chars), 16, len(chars))\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=[char_to_idx[ch] for ch in data[:-1]]\n",
    "targets=[char_to_idx[ch] for ch in data[1:]]\n",
    "\n",
    "inputs=torch.tensor(inputs,dtype=torch.long).view(-1,1)\n",
    "inputs=nn.functional.one_hot(inputs,num_classes=len(chars)).float()\n",
    "\n",
    "targets=torch.tensor(targets,dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TRAINING THE MODEL WITH 100 EPOCHS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 2.1541919708251953\n",
      "epoch 200, loss 2.0188331604003906\n",
      "epoch 300, loss 1.9942307472229004\n",
      "epoch 400, loss 1.986881971359253\n",
      "epoch 500, loss 1.9836246967315674\n",
      "epoch 600, loss 1.9818694591522217\n",
      "epoch 700, loss 1.9808194637298584\n",
      "epoch 800, loss 1.9801398515701294\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for epoch in range(800):\n",
    "    model.train()\n",
    "    outputs=model(inputs)\n",
    "    loss=criterion(outputs,targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    i+=1\n",
    "    if(i%100==0):\n",
    "        print(f\"epoch {epoch+1}, loss {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **TESTING THE MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_input=char_to_idx['S']\n",
    "test_input=nn.functional.one_hot(torch.tensor(test_input).view(-1,1),num_classes=len(chars)).float()\n",
    "pred_output=model(test_input)\n",
    "pred_char=torch.argmax(pred_output,1).item()\n",
    "pred_char = idx_to_char[pred_char]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n"
     ]
    }
   ],
   "source": [
    "print(pred_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **WORD LEVEL PREDICTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"When it comes to generating text, GANs and LSTMs have different approaches. LSTMs excel at capturing sequential patterns and context, making them well-suited for tasks like language translation and text summarization. However, they can struggle with creativity and diversity in their output. On the other hand, GANs are designed to generate novel and diverse text by learning the underlying data distribution. While they can produce more creative content, GANs can be challenging to train and evaluate, and may require additional techniques to ensure coherence and fluency. Ultimately, the choice between GANs and LSTMs depends on the specific text generation task and the desired output: if you need coherent and natural-sounding text, LSTMs might be the better choice, but if you want to generate creative and diverse content, GANs could be the way to go.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.translate(str.maketrans('', '', string.punctuation))\n",
    "words=word_tokenize(data)\n",
    "vocab=list(set(words))\n",
    "words_to_idx={word:i for i,word in enumerate(vocab)}\n",
    "idx_to_words={i:word for i,word in enumerate(vocab)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "model2 = RNNModel(input_size=vocab_size,hidden_size= 16, output_size=vocab_size)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model2.parameters(),lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "word_inputs = [words_to_idx.get(ch, -1) for ch in data.split()[:-1]]  \n",
    "word_targets = [words_to_idx.get(ch, -1) for ch in data.split()[1:]]\n",
    "\n",
    "\n",
    "word_inputs = [x for x in word_inputs if x != -1]\n",
    "word_targets = [x for x in word_targets if x != -1]\n",
    "\n",
    "\n",
    "word_inputs = torch.tensor(word_inputs, dtype=torch.long).view(-1, 1)\n",
    "word_targets = torch.tensor(word_targets, dtype=torch.long)\n",
    "\n",
    "# One-hot encoding\n",
    "word_inputs = nn.functional.one_hot(word_inputs, num_classes=vocab_size).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100, loss 0.7227327227592468\n",
      "epoch 200, loss 0.6480339169502258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 300, loss 0.6387345194816589\n",
      "epoch 400, loss 0.6353968381881714\n",
      "epoch 500, loss 0.6337555646896362\n",
      "epoch 600, loss 0.6328078508377075\n",
      "epoch 700, loss 0.6322029232978821\n",
      "epoch 800, loss 0.6317892670631409\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for epoch in range(800):\n",
    "    model2.train()\n",
    "    word_outputs=model2(word_inputs)\n",
    "    loss=criterion(word_outputs,word_targets)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    i+=1\n",
    "    if(i%100==0):\n",
    "        print(f\"epoch {epoch+1}, loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(word:str):\n",
    "    model2.eval()\n",
    "    test_input=words_to_idx[word]\n",
    "    test_input=nn.functional.one_hot(torch.tensor(test_input).view(-1,1),num_classes=len(vocab)).float()\n",
    "    pred_output=model2(test_input)\n",
    "    pred_char=torch.argmax(pred_output,1).item()\n",
    "    pred_char = idx_to_words[pred_char]\n",
    "    return pred_char\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word is 'language' and predicted next word is 'translation' \n",
      "Input word is 'they' and predicted next word is 'can' \n",
      "Input word is 'way' and predicted next word is 'to' \n",
      "Input word is 'learning' and predicted next word is 'the' \n",
      "Input word is 'to' and predicted next word is 'generate' \n",
      "Input word is 'train' and predicted next word is 'and' \n",
      "Input word is 'like' and predicted next word is 'language' \n",
      "Input word is 'the' and predicted next word is 'choice' \n",
      "Input word is 'other' and predicted next word is 'hand' \n",
      "Input word is 'tasks' and predicted next word is 'like' \n",
      "Input word is 'capturing' and predicted next word is 'sequential' \n",
      "Input word is 'creative' and predicted next word is 'content' \n",
      "Input word is 'additional' and predicted next word is 'techniques' \n",
      "Input word is 'are' and predicted next word is 'designed' \n",
      "Input word is 'their' and predicted next word is 'output' \n",
      "Input word is 'want' and predicted next word is 'to' \n",
      "Input word is 'excel' and predicted next word is 'at' \n",
      "Input word is 'in' and predicted next word is 'their' \n",
      "Input word is 'go' and predicted next word is 'and' \n",
      "Input word is 'context' and predicted next word is 'making' \n",
      "Input word is 'generate' and predicted next word is 'creative' \n",
      "Input word is 'output' and predicted next word is 'if' \n",
      "Input word is 'for' and predicted next word is 'tasks' \n",
      "Input word is 'approaches' and predicted next word is 'LSTMs' \n",
      "Input word is 'patterns' and predicted next word is 'and' \n",
      "Input word is 'underlying' and predicted next word is 'data' \n",
      "Input word is 'content' and predicted next word is 'GANs' \n",
      "Input word is 'you' and predicted next word is 'want' \n",
      "Input word is 'distribution' and predicted next word is 'While' \n",
      "Input word is 'struggle' and predicted next word is 'with' \n",
      "Input word is 'by' and predicted next word is 'learning' \n",
      "Input word is 'and' and predicted next word is 'diverse' \n",
      "Input word is 'on' and predicted next word is 'the' \n",
      "Input word is 'coherent' and predicted next word is 'and' \n",
      "Input word is 'GANs' and predicted next word is 'and' \n",
      "Input word is 'diversity' and predicted next word is 'in' \n",
      "Input word is 'with' and predicted next word is 'creativity' \n",
      "Input word is 'generation' and predicted next word is 'task' \n",
      "Input word is 'fluency' and predicted next word is 'Ultimately' \n",
      "Input word is 'require' and predicted next word is 'additional' \n",
      "Input word is 'depends' and predicted next word is 'on' \n",
      "Input word is 'might' and predicted next word is 'be' \n",
      "Input word is 'comes' and predicted next word is 'to' \n",
      "Input word is 'more' and predicted next word is 'creative' \n",
      "Input word is 'diverse' and predicted next word is 'content' \n",
      "Input word is 'may' and predicted next word is 'require' \n",
      "Input word is 'different' and predicted next word is 'approaches' \n",
      "Input word is 'creativity' and predicted next word is 'and' \n",
      "Input word is 'text' and predicted next word is 'by' \n",
      "Input word is 'if' and predicted next word is 'you' \n",
      "Input word is 'However' and predicted next word is 'they' \n",
      "Input word is 'making' and predicted next word is 'them' \n",
      "Input word is 'ensure' and predicted next word is 'coherence' \n",
      "Input word is 'naturalsounding' and predicted next word is 'text' \n",
      "Input word is 'While' and predicted next word is 'they' \n",
      "Input word is 'LSTMs' and predicted next word is 'excel' \n",
      "Input word is 'challenging' and predicted next word is 'to' \n",
      "Input word is 'novel' and predicted next word is 'and' \n",
      "Input word is 'it' and predicted next word is 'comes' \n",
      "Input word is 'them' and predicted next word is 'wellsuited' \n",
      "Input word is 'produce' and predicted next word is 'more' \n",
      "Input word is 'data' and predicted next word is 'distribution' \n",
      "Input word is 'be' and predicted next word is 'the' \n",
      "Input word is 'wellsuited' and predicted next word is 'for' \n",
      "Input word is 'could' and predicted next word is 'be' \n",
      "Input word is 'When' and predicted next word is 'it' \n",
      "Input word is 'better' and predicted next word is 'choice' \n",
      "Input word is 'at' and predicted next word is 'capturing' \n",
      "Input word is 'specific' and predicted next word is 'text' \n",
      "Input word is 'techniques' and predicted next word is 'to' \n",
      "Input word is 'but' and predicted next word is 'if' \n",
      "Input word is 'hand' and predicted next word is 'GANs' \n",
      "Input word is 'Ultimately' and predicted next word is 'the' \n",
      "Input word is 'task' and predicted next word is 'and' \n",
      "Input word is 'desired' and predicted next word is 'output' \n",
      "Input word is 'between' and predicted next word is 'GANs' \n",
      "Input word is 'sequential' and predicted next word is 'patterns' \n",
      "Input word is 'choice' and predicted next word is 'but' \n",
      "Input word is 'On' and predicted next word is 'the' \n",
      "Input word is 'evaluate' and predicted next word is 'and' \n",
      "Input word is 'coherence' and predicted next word is 'and' \n",
      "Input word is 'need' and predicted next word is 'coherent' \n",
      "Input word is 'generating' and predicted next word is 'text' \n",
      "Input word is 'have' and predicted next word is 'different' \n",
      "Input word is 'translation' and predicted next word is 'and' \n",
      "Input word is 'can' and predicted next word is 'be' \n",
      "Input word is 'summarization' and predicted next word is 'However' \n",
      "Input word is 'designed' and predicted next word is 'to' \n"
     ]
    }
   ],
   "source": [
    "for i in vocab:\n",
    "    output_pred=predict_word(i)\n",
    "    print(f\"Input word is '{i}' and predicted next word is '{output_pred}' \" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **NEXT SENTENCE GEENRATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('alice.txt','r',encoding='utf-8') as file:\n",
    "    text=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=text.split()\n",
    "word_count=Counter(words)\n",
    "vocab=list(word_count.keys())\n",
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_idx={i:word for word,i in enumerate(vocab)}\n",
    "idx_to_Word={word:i for word,i in enumerate(vocab)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 64\n",
    "samples = [words[i:i+SEQUENCE_LENGTH+1] for i in range(len(words)-SEQUENCE_LENGTH)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textloader(Dataset):\n",
    "    def __init__(self,samples,word_to_idx):\n",
    "        self.samples=samples\n",
    "        self.word_to_idx=word_to_idx\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        samples=self.samples[idx]\n",
    "        input_seq=torch.LongTensor([self.word_to_idx[word] for word in samples[:-1]])\n",
    "        target_seq=torch.LongTensor([self.word_to_idx[word] for word in samples[1:]])\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "        19, 20, 21, 22, 23, 19, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 31,\n",
      "        35, 13, 36, 15, 13, 37, 38, 39, 40, 22, 41, 10, 33, 42, 18, 43, 44,  2,\n",
      "         3,  4, 45, 46,  7,  8, 47, 48, 49, 50]), tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
      "        20, 21, 22, 23, 19, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 31, 35,\n",
      "        13, 36, 15, 13, 37, 38, 39, 40, 22, 41, 10, 33, 42, 18, 43, 44,  2,  3,\n",
      "         4, 45, 46,  7,  8, 47, 48, 49, 50, 51]))\n"
     ]
    }
   ],
   "source": [
    "batch_size=12\n",
    "dataset=textloader(samples,word_to_idx)\n",
    "dataloader=DataLoader(dataset,batch_size=batch_size,shuffle=True)\n",
    "print(dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerationModel(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_size,num_layers):\n",
    "        super(TextGenerationModel, self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.lstm=nn.LSTM(input_size=embedding_dim,hidden_size=hidden_size,num_layers=num_layers,batch_first=True)\n",
    "        self.fc=nn.Linear(hidden_size,vocab_size)\n",
    "        self.hidden_size=hidden_size\n",
    "        self.num_layers=num_layers\n",
    "        \n",
    "    def forward(self,x,hidden=None):\n",
    "        if hidden==None:\n",
    "            hidden=self.init_hidden(x.shape[0])\n",
    "        x=self.embedding(x)\n",
    "        out,(h_n,c_n)=self.lstm(x,hidden)\n",
    "        out=out.contiguous().view(-1,self.hidden_size)\n",
    "        out=self.fc(out)\n",
    "        return out,(h_n,c_n)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return h0, c0\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "learning_rate = 0.01\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model=TextGenerationModel(vocab_size,embedding_dim,hidden_size,num_layers).to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters(),lr=learning_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,epochs,dataloader,criterion,optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss=0\n",
    "        for input_Seq,target_Seq in dataloader:\n",
    "            input_Seq,target_Seq=input_Seq.to(device),target_Seq.to(device)\n",
    "            outputs,_=model(input_Seq)\n",
    "            loss=criterion(outputs,target_Seq.view(-1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss+=loss.detach().cpu().numpy()\n",
    "        epoch_loss /= len(dataloader)\n",
    "        print(f\"Epoch {epoch} loss: {epoch_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 3.547\n",
      "Epoch 1 loss: 1.931\n",
      "Epoch 2 loss: 1.524\n",
      "Epoch 3 loss: 1.336\n",
      "Epoch 4 loss: 1.226\n",
      "Epoch 5 loss: 1.154\n",
      "Epoch 6 loss: 1.105\n",
      "Epoch 7 loss: 1.066\n",
      "Epoch 8 loss: 1.040\n",
      "Epoch 9 loss: 1.014\n",
      "Epoch 10 loss: 0.993\n",
      "Epoch 11 loss: 0.976\n",
      "Epoch 12 loss: 0.959\n",
      "Epoch 13 loss: 0.949\n",
      "Epoch 14 loss: 0.934\n",
      "Epoch 15 loss: 0.923\n",
      "Epoch 16 loss: 0.910\n",
      "Epoch 17 loss: 0.907\n",
      "Epoch 18 loss: 0.896\n",
      "Epoch 19 loss: 0.890\n",
      "Epoch 20 loss: 0.883\n",
      "Epoch 21 loss: 0.875\n",
      "Epoch 22 loss: 0.869\n",
      "Epoch 23 loss: 0.867\n",
      "Epoch 24 loss: 0.857\n",
      "Epoch 25 loss: 0.855\n",
      "Epoch 26 loss: 0.850\n",
      "Epoch 27 loss: 0.843\n",
      "Epoch 28 loss: 0.846\n",
      "Epoch 29 loss: 0.834\n",
      "Epoch 30 loss: 0.832\n",
      "Epoch 31 loss: 0.829\n",
      "Epoch 32 loss: 0.824\n",
      "Epoch 33 loss: 0.821\n",
      "Epoch 34 loss: 0.821\n",
      "Epoch 35 loss: 0.815\n",
      "Epoch 36 loss: 0.811\n",
      "Epoch 37 loss: 0.811\n",
      "Epoch 38 loss: 0.810\n",
      "Epoch 39 loss: 0.806\n",
      "Epoch 40 loss: 0.802\n",
      "Epoch 41 loss: 0.798\n",
      "Epoch 42 loss: 0.798\n",
      "Epoch 43 loss: 0.800\n",
      "Epoch 44 loss: 0.789\n",
      "Epoch 45 loss: 0.792\n",
      "Epoch 46 loss: 0.791\n",
      "Epoch 47 loss: 0.783\n",
      "Epoch 48 loss: 0.782\n",
      "Epoch 49 loss: 0.782\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train(model,epochs,dataloader,criterion)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'text generator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "geenratory=torch.load('text generator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text is:  unless it was all ridges and furrows; the balls were live hedgehogs, the mallets live flamingoes, and the soldiers were silent, and looked at her or three soldiers were silent, and looked at each other of the house opened, and a large plate came skimming out, straight at the Footman's head: it just grazed his nose, and broke to pieces against one of the trees behind him. '--or next day, maybe,' the Footman continued in a fight with the middle of one! There ought to have finished,' said the King. 'When did you begin?' The Hatter she felt that she had not like to get a little.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(geenratory,start,num_words):\n",
    "    geenratory.eval()\n",
    "    words=start.split()\n",
    "    for _ in range(num_words):\n",
    "        input_seq=torch.LongTensor([word_to_idx[word] for word in words[-SEQUENCE_LENGTH:]]).unsqueeze(0).to(device)\n",
    "        h,c=geenratory.init_hidden(1)\n",
    "        output,(h,c)=geenratory(input_seq,(h,c))\n",
    "        next_token=output.argmax(1)[-1].item()\n",
    "        words.append(idx_to_Word[next_token])\n",
    "        \n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "\n",
    "print('Generated text is: ',generate_text(model,'unless it was all ridges and furrows;',num_words=100))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text is:  Alice was a child,' said to the jury. 'Not yet, not yet!' the Rabbit hastily interrupted. 'There's a great deal to eat or drink under the circumstances. There was a little bottle that stood near the looking-glass. There was no label this time with the words 'DRINK ME,' but nevertheless she uncorked it and put it a little queer, won't you?' 'Not a bit,' said the Caterpillar. 'Well, perhaps your feelings may be different,' said Alice; 'all I know is, you please, sir--' The Rabbit started violently, dropped the white kid gloves and the fan, and skurried away into the darkness as hard\n"
     ]
    }
   ],
   "source": [
    "print('Generated text is: ',generate_text(model,'Alice was a',num_words=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Fine Tuning the Model for better Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the Optimizer and Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers=optim.Adam(model.parameters(),lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.782\n",
      "Epoch 1 loss: 0.779\n",
      "Epoch 2 loss: 0.783\n",
      "Epoch 3 loss: 0.780\n",
      "Epoch 4 loss: 0.771\n",
      "Epoch 5 loss: 0.777\n",
      "Epoch 6 loss: 0.772\n",
      "Epoch 7 loss: 0.777\n",
      "Epoch 8 loss: 0.769\n",
      "Epoch 9 loss: 0.765\n",
      "Epoch 10 loss: 0.769\n",
      "Epoch 11 loss: 0.764\n",
      "Epoch 12 loss: 0.765\n",
      "Epoch 13 loss: 0.771\n",
      "Epoch 14 loss: 0.761\n"
     ]
    }
   ],
   "source": [
    "epochss=15\n",
    "train(model,epochss,dataloader,criterion,optimizers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text is:  On this the White Rabbit gave a little scream of laughter. 'Oh, hush!' the Rabbit whispered in a frightened tone. 'The Queen will hear you! You see, she looked much far about the Dormouse said--' the Hatter said, tossing his head contemptuously. 'I dare say you say even when it's pleased. Now I growl when I'm pleased, and wag my tail when I'm angry. Therefore I'm mad.' 'I call it purring, not growling,' said Alice. 'Call it that stood made out that it had been. But a box of comfits, (luckily the salt water had not feel encouraged to ask any more questions I should\n"
     ]
    }
   ],
   "source": [
    "print('Generated text is: ',generate_text(model,'On this the White Rabbit',num_words=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text is:  the rabbit with either a waistcoat-pocket, or a watch to have no time when he met in the house, \"Let us both go and live at the number of executions Alice cautiously bottle was beginning to get to,' said the Cat. 'I don't much care where--' said Alice. 'Then it doesn't matter which way you go,' said the Cat. '--so long as she would gather about her other into a small passage, not much larger than in her and then them free, Exactly as we were. My notion and said severely 'Who is this?' She said the Duchess, 'as pigs have to\n"
     ]
    }
   ],
   "source": [
    "print('Generated text is: ',generate_text(model,'the rabbit',num_words=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text is:  can I shouldn't like THAT!' 'Oh, I wish you could tell you had been looked up, and there stood the same, shedding gallons of tears, until there was no more and seemed every way, and then said the Mouse heard one who you tell of her going, though she looked back once its legs hanging down, but generally, just as she had to kneel down on the floor: in another minute this Alice as she could do, lying down into the darkness as hard as she could guess, she was now about two feet high, and was going to dive in among\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Generated text is: ',generate_text(model,'can I' ,num_words=100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
